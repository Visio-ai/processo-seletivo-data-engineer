
# Case de Engenharia de Dados

Este reposit√≥rio cont√©m um case voltado para opera√ß√µes de dados. O objetivo principal n√£o √© demonstrar o uso de ferramentas espec√≠ficas, mas sim enfatizar a tomada de decis√µes estrat√©gicas e a dissemina√ß√£o de boas pr√°ticas aplic√°veis a uma audi√™ncia ampla.

A parte t√©cnica utiliza tecnologias amplamente conhecidas e acess√≠veis, com grande suporte em f√≥runs e ferramentas como IA. Na Visio, acreditamos que a capacidade de pesquisa e aprendizado cont√≠nuo √© muito mais valiosa do que o dom√≠nio exclusivo de uma ferramenta. Valorizamos profissionais curiosos, que se interessam por explorar novas tecnologias e encontrar solu√ß√µes inovadoras.

Este case √© inspirado em um cen√°rio realista, que reflete desafios iniciais t√≠picos enfrentados por engenheiros de dados. Algumas complexidades foram simplificadas para alinhar o case √†s demandas e restri√ß√µes de tempo comuns no final do ano, permitindo um foco mais pr√°tico e direto.


## Considera√ß√µes importantes
- O Postgres ser√° utilizado como banco OLAP para este exerc√≠cio.
- Em cen√°rios reais, paga-se tanto pelo armazenamento quanto pela recupera√ß√£o de dados em queries.
- O custo das queries geralmente √© significativamente mais alto que o de armazenamento, tornando essencial a otimiza√ß√£o das consultas para minimizar custos.
- Os times de neg√≥cio consultam os dados via dashboards v√°rias vezes ao dia, resultando em uma alta frequ√™ncia de execu√ß√£o de queries.
- Apesar de t√©cnicas como particionamento e clusteriza√ß√£o serem √∫teis, o foco deste case est√° na organiza√ß√£o de tabelas para consultas eficientes e inteligentes.
- Documenta√ß√£o clara √© fundamental: m√∫ltiplas equipes trabalham em diferentes contextos simultaneamente, tornando playbooks e documenta√ß√£o escrita essenciais para alinhar solu√ß√µes.

## Carregando os dados raw
Nesta etapa, faremos a extra√ß√£o dos dados. Para simplificar, os dados foram disponibilizados em formato JSON, eliminando a necessidade de intera√ß√£o com APIs. Eles representam cupons fiscais de uma loja fict√≠cia.

Os dados de cupons fiscais s√£o a base da maioria das an√°lises realizadas para diversos clientes. Geralmente, esses dados est√£o em bancos n√£o relacionais, mas, neste case, est√£o em arquivos para facilitar a manipula√ß√£o. Cada arquivo cont√©m uma lista de cupons vendidos em um dia. Abaixo, segue a descri√ß√£o dos campos:

```json
{
    "coupon": "1AM5250082", //ID do cupom, √© √∫nico nesse caso em particular
    "date_time": "20/12/2023 06:45:00", // Datetime do hor√°rio local de estra√ß√£o
    "price_total": "40.00", // O pre√ßo total do cupom fiscal
    "payments": [ // aqui √© uma lista pq pode haver m√∫ltiplos meios de pagamento
    {
        "type": "Dinheiro", //Tipo do pagmento
        "amount": "40.00" // valor pago
    }
    ],
    "timestamp_erp": 1703054700, //timestamp do cupom
    "details": { //esses detalhes podem ser ignorados para essa atividade, mas eles representa informa√ß√µes sobre o computador que emitiu a nota fiscal
        "movimento": "",
        "operator": "00256",
        "abertura": "",
        "pdv": "14"
    },
    "fee": "4.29", //Taxas pagas no geral
    "delivery": false, //Se o cupom √© delivery
    "staff": false, // Se o cupom √© dos funcion√°rios
    "items": [ // lista do itens presentes na loja
        {
            "item": "Sandu√≠che Bauru", // nome do item
            "canceled": "false", //Se foi cancelado
            "total": "20.00", // Valor total dos itens desse tipo
            "price": "20.00", // Pre√ßo individual do item
            "quantity": 1 // Quantidade comprada
        }
    ]
}
```
### Requisitos para Carregamento
- Os dados brutos devem ser carregados no Postgres.
- Escolha a ferramenta ou m√©todo que preferir (ex.: scripts em Python ou aplica√ß√µes).
- Regras:
    - Cupons iniciados com 4 indicam delivery.
    - Cupons iniciados com 3 indicam autoatendimento.
    - Cupons iniciados com 1 indicam consumo no restaurante.

```
‚ö†Ô∏è O ambiente j√° est√° configurado com Docker Compose para subir o Spark e o banco de dados Postgres, garantindo a replicabilidade do projeto.
```

## Transformando os dados do banco
Nesta etapa, utilizaremos o Spark para transformar os dados. Embora o volume de dados seja pequeno, o objetivo √© simular um cen√°rio de grande escala, onde fluxos cont√≠nuos de dados precisam ser processados.

O foco aqui √© demonstrar a utiliza√ß√£o de uma ferramenta robusta e popular de pipelines de dados.


### Checando se o spark est√° funcionado
Ap√≥s subir o compose, entre dentro do container executando o comando a seguir:
```bash
sudo docker compose exec -it spark bash
```

Dentro do container, √© poss√≠vel executar o exemplo que deixei pronto, com o seguinte comando:
```bash
spark-submit  /opt/spark-apps/example.py
```

## Perguntas que o time de neg√≥cio geralmente quer responder

Nosso objetivo √© auxiliar o time de neg√≥cios a obter insights significativos. As perguntas mais comuns incluem:

- Qual √© o faturamento da loja em diferentes per√≠odos de tempo (di√°rio, semanal e mensal)?
- Quais s√£o os itens mais vendidos e menos vendidos? Quais itens geram mais faturamento?
- Quais s√£o os m√©todos de pagamento mais utilizados?
- Quais s√£o os hor√°rios de maior movimento?
- Qual √© a divis√£o de vendas por modalidade (delivery, restaurante e autoatendimento)?

üí° Nota: √â importante garantir que essas perguntas possam ser respondidas com consultas eficientes e que minimizem a quantidade de dados processados.



### Visualizando os dados
Use a ferramenta de visualiza√ß√£o de sua prefer√™ncia (ex.: Tableau, Metabase, Grafana). Adicione-a ao docker-compose.yml para que possa ser executada localmente e visualizar os dashboards.

‚ö†Ô∏è Observa√ß√£o: O formato dos gr√°ficos n√£o √© o foco; o importante √© verificar o funcionamento das queries com seletores de datas.

 ## Submiss√£o e o que ser√° avaliado
- Caso o reposit√≥rio esteja privado, adicione-me como colaborador para que eu possa clonar o projeto.
- A avalia√ß√£o seguir√° a documenta√ß√£o fornecida para replicar o ambiente e verificar os resultados.

 ### Principais pontos de avalia√ß√£o
1. Qualidade da documenta√ß√£o, especialmente para desenvolvedores.
2. Organiza√ß√£o e transforma√ß√£o dos dados no pipeline.
3. Estrutura e organiza√ß√£o das tabelas geradas.
4. Explica√ß√£o clara das escolhas realizadas.
5. Documenta√ß√£o voltada ao time de neg√≥cios para utiliza√ß√£o das tabelas.
